# ‚úÖ TIMESTAMP FORMATTING FIX - APPLIED

**Date:** 2025-10-06
**Issue:** BigQuery load failures due to malformed timestamps
**Status:** FIXED

---

## üêõ Problem

LLM was generating timestamps in format that BigQuery rejects:
- **Wrong:** `'2024-05-21 T11:20:05 UTC'` (space before T, UTC suffix)
- **Correct:** `'2024-05-21 11:20:05'` (BigQuery standard format)

This caused 3 tables (`users`, `transactions`, `search_queries`) to fail loading with 100+ parsing errors.

---

## ‚úÖ Solution Implemented

### 1. **Added `_fix_bigquery_timestamps()` Method**
Location: `backend/agentic_service/agents/synthetic_data_generator_optimized.py:900`

**Fixes Applied:**
- ‚úÖ Remove space before 'T': `'2024-05-21 T11:20:05'` ‚Üí `'2024-05-21T11:20:05'`
- ‚úÖ Remove ' UTC' suffix: `'2024-05-21T11:20:05 UTC'` ‚Üí `'2024-05-21 11:20:05'`
- ‚úÖ Standardize format to BigQuery spec: `'YYYY-MM-DD HH:MM:SS'`
- ‚úÖ Update old dates to current year (2024) to align with golden queries

### 2. **Integrated into LLM Data Generation Pipeline**
Location: `backend/agentic_service/agents/synthetic_data_generator_optimized.py:656`

After LLM generates data and before CSV write:
```python
# Create DataFrame
df = pd.DataFrame(data_array)

# FIX: Clean up timestamps to ensure BigQuery compatibility
df = self._fix_bigquery_timestamps(df, schema_fields)
```

### 3. **Enhanced LLM Prompt**
Location: `backend/agentic_service/agents/synthetic_data_generator_optimized.py:509`

Added explicit instruction:
```
**CRITICAL - DATES**: Use CURRENT YEAR (2024) for all dates.
Most dates should be recent (last 12 months).
Format: YYYY-MM-DD HH:MM:SS (NO 'T', NO 'UTC')
```

### 4. **Switched Orchestrator to Optimized Version**
Location: `backend/agentic_service/demo_orchestrator.py:91`

**Before:**
```python
# USING MARKDOWN VERSION with foreign key fixes
from agentic_service.agents.synthetic_data_generator_markdown import ...
```

**After:**
```python
# USING OPTIMIZED VERSION with timestamp fixes and story-driven generation
from agentic_service.agents.synthetic_data_generator_optimized import ...
```

---

## üéØ Expected Outcomes

1. **All tables load successfully** to BigQuery (no more 0-row tables)
2. **Timestamps are properly formatted** (`YYYY-MM-DD HH:MM:SS`)
3. **Dates align with current year** (2024) for golden query compatibility
4. **Golden queries return results** instead of empty sets

---

## üìù Technical Details

### Timestamp Processing Logic

```python
def _fix_bigquery_timestamps(df, schema_fields):
    for field in schema_fields:
        if field['type'] in ['TIMESTAMP', 'DATE', 'DATETIME']:
            for each value:
                1. Remove space before T: re.sub(r'\s+T', 'T', val)
                2. Remove UTC suffix: val.replace(' UTC', '')
                3. Parse datetime: pd.to_datetime(val)
                4. Update to current year if old
                5. Format to BigQuery standard:
                   - TIMESTAMP: '%Y-%m-%d %H:%M:%S'
                   - DATE: '%Y-%m-%d'
```

### Date Range Alignment

**Problem:**
- Golden Query 1: `WHERE DATE_TRUNC(transaction_date, MONTH) = DATE '2025-10-01'`
- Old Data: Transactions from 2023-09-28, 2023-10-12 (NO MATCH!)

**Solution:**
- Auto-update dates older than 1 year to current year (2024)
- Keep month/day same, just shift year
- Ensures queries searching "this month", "last quarter" find data

---

## üß™ Testing

### Before Fix:
```bash
$ bq query "SELECT COUNT(*) FROM offerup_capi_demo_20251006.users"
Result: 0 rows

$ bq query "SELECT COUNT(*) FROM offerup_capi_demo_20251006.transactions"
Result: 0 rows
```

### After Fix (Expected):
```bash
$ bq query "SELECT COUNT(*) FROM <new_dataset>.users"
Result: 3500 rows

$ bq query "SELECT COUNT(*) FROM <new_dataset>.transactions"
Result: 15000 rows
```

---

## üöÄ Next Steps

1. **Test with new provisioning request:**
   ```bash
   curl -X POST http://localhost:8000/api/provision/start \
     -H "Content-Type: application/json" \
     -d '{"customer_url": "https://www.offerup.com"}'
   ```

2. **Verify table row counts** after generation

3. **Run golden queries** to confirm meaningful results

4. **Monitor logs** for any timestamp parsing warnings

---

## üìä Story-Driven Data Generation

The optimized generator ALREADY supports story-driven data:
- ‚úÖ LLM prompt includes **golden queries** from demo_story
- ‚úÖ LLM prompt includes **business challenges** and **demo narrative**
- ‚úÖ LLM prompt includes **talking track** and **key metrics**
- ‚úÖ LLM generates data patterns that align with business story

**No complex multi-phase system needed** - just fix the timestamp bug and let the existing LLM do its job!

---

**Status: Ready for Testing**
